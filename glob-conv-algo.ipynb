{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random, math\n",
    "import matlab\n",
    "import matlab.engine as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyper-Parameters\n",
    "n = 2000 # number of samples\n",
    "d = 30  # input dimension\n",
    "k = 5   # hidden layer size\n",
    "kappa = 2 \n",
    "lr = 0.02\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Activation Function\n",
    "def phi_s(h):\n",
    "    return h**2 if h > 0 else 0\n",
    "phi = np.vectorize(phi_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data and Params Generation\n",
    "\n",
    "gauss_mat_u = np.random.normal(0.0, 0.1 , (d,k))\n",
    "gauss_mat_v = np.random.normal(0.0, 0.1 , (k,k))\n",
    "\n",
    "U, temp = np.linalg.qr(gauss_mat_u)\n",
    "V, temp = np.linalg.qr(gauss_mat_v)\n",
    "\n",
    "U, V = np.linalg.qr(gauss_mat_u)\n",
    "diag = []\n",
    "v_gt = []\n",
    "v_choice = [1,-1]\n",
    "for iter in range(k):\n",
    "    diag.append(1+1.*iter*(kappa-1)/(k-1))\n",
    "    v_gt.append(random.choice(v_choice))\n",
    "    \n",
    "Sigma = np.diag(diag)\n",
    "W_gt = np.dot(np.dot(U, Sigma), np.transpose(V))\n",
    "v_gt = np.asarray(v_gt)\n",
    "train_x = []\n",
    "train_y = []\n",
    "for iter in range(n):\n",
    "    train_x.append(np.random.normal(0.0,1.,d))\n",
    "    train_y.append(np.dot(phi(np.dot(train_x[iter], W_gt)),v_gt))\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.transpose(np.asarray(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## moments\n",
    "\n",
    "def gamma(j,sigma):\n",
    "    estm = 0\n",
    "    for i in range(10000):\n",
    "        z = np.random.normal(0.0, 1.0)\n",
    "        estm += phi_s(z*sigma)*(z**j)\n",
    "    return estm/10000\n",
    "\n",
    "m = np.zeros((4,k))\n",
    "for i in range(k):\n",
    "    m[0,i] = gamma(1,np.linalg.norm(W_gt[:,i]))\n",
    "    m[1,i] = gamma(2,np.linalg.norm(W_gt[:,i])) - gamma(0,np.linalg.norm(W_gt[:,i]))\n",
    "    m[2,i] = gamma(3,np.linalg.norm(W_gt[:,i])) - 3*gamma(1,np.linalg.norm(W_gt[:,i]))\n",
    "    m[3,i] = gamma(4,np.linalg.norm(W_gt[:,i])) + 3*gamma(0,np.linalg.norm(W_gt[:,i])) - 6*gamma(2,np.linalg.norm(W_gt[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outer3(a,b,c):\n",
    "    A = np.outer(a,b)\n",
    "    B = []\n",
    "    for third in c:\n",
    "        B.append(A*third)\n",
    "    return np.asarray(B)\n",
    "\n",
    "def outer4(a,b,c,d):\n",
    "    A = outer3(a,b,c)\n",
    "    B = []\n",
    "    for fourth in d:\n",
    "        B.append(A*fourth)\n",
    "    return np.asarray(B)\n",
    "\n",
    "def outer3I(x):\n",
    "    return outer3(x,x,x)\n",
    "\n",
    "def outer4I(x):\n",
    "    return outer4(x,x,x,x)\n",
    "\n",
    "def specOuterI(x):\n",
    "    d = len(x)\n",
    "    iden = np.identity(d)\n",
    "    final = np.zeros([d,d,d])\n",
    "    for i in range(d):\n",
    "        final += outer3(x, iden[i], iden[i]) + outer3(iden[i], x, iden[i])+ outer3(iden[i], iden[i],x)\n",
    "    return final\n",
    "\n",
    "def specOuterMat(M):\n",
    "    d = np.shape(M)[0]\n",
    "    ## TODO\n",
    "    return np.zeros([d,d,d,d])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m[2,3]/np.linalg.norm(W_gt[:,3])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multLnr(M, argList):\n",
    "    if len(np.shape(M)) == 3:\n",
    "        a,b,c = argList\n",
    "\n",
    "        assert(np.shape(M)[0] == np.shape(a)[0])\n",
    "        assert(np.shape(M)[1] == np.shape(b)[0])\n",
    "        assert(np.shape(M)[2] == np.shape(c)[0])\n",
    "        ## HardCoding Here :: TODO\n",
    "        res = np.zeros([np.shape(a)[-1],np.shape(b)[-1] ])\n",
    "        for itera in range(np.shape(a)[0]):\n",
    "            for iterb in range(np.shape(b)[0]):\n",
    "                for iterc in range(np.shape(c)[0]):\n",
    "                    res += M[itera, iterb, iterc]*c[iterc]*np.outer(a[itera], b[iterb])\n",
    "        return res\n",
    "    else:\n",
    "        assert(len(np.shape(M)) == 4)\n",
    "        a,b,c,d = argList\n",
    "        if len(np.shape(c)) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            res = np.zeros([np.shape(a)[-1],np.shape(b)[-1] ])\n",
    "            for itera in range(np.shape(a)[0]):\n",
    "                for iterb in range(np.shape(b)[0]):\n",
    "                    for iterc in range(np.shape(c)[0]):\n",
    "                        for iterd in range(np.shape(d)[0]):\n",
    "                            res += M[itera, iterb, iterc]*c[iterc]*d[iterd]*np.outer(a[itera], b[iterb])\n",
    "            return res\n",
    "        \n",
    "def multLnr1(M, V):\n",
    "    d = np.shape(M)[0]\n",
    "    k = np.shape(V)[1]\n",
    "    \n",
    "    res = np.zeros((k,k))\n",
    "    \n",
    "    for itera in range(d):\n",
    "        for iterb in range(d):\n",
    "            res += M[itera, iterb]*np.outer(V[itera], V[iterb])\n",
    "            \n",
    "    return res\n",
    "\n",
    "def multLnr2(P3, V):\n",
    "    d = np.shape(P3)[0]\n",
    "    k = np.shape(V)[1]\n",
    "    \n",
    "    res = np.zeros((k,k,k))\n",
    "    \n",
    "    for itera in range(d):\n",
    "        for iterb in range(d):\n",
    "            for iterc in range(d):\n",
    "                res += P3[itera, iterb, iterc]*outer3(V[itera], V[iterb], V[iterc])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def prob(x):\n",
    "#     return math.pow(math.e, -0.5*np.dot(np.transpose(x),x))/np.sqrt(math.pow(2*math.pi, len(x)))\n",
    "\n",
    "def getM1(X, y):\n",
    "    M1 = np.zeros(d)\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M1 += y[iter]*X[iter]\n",
    "    return M1/len(y)\n",
    "\n",
    "def getM2(X, y):\n",
    "    M2 = np.zeros([d,d])\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M2 += y[iter]*(np.outer(X[iter], X[iter]) - np.identity(d))\n",
    "    return M2/len(y)\n",
    "\n",
    "def getM3(X, y):\n",
    "    M3 = np.zeros([d,d,d])\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M3 += y[iter]*(outer3I(X[iter]) - specOuterI(X[iter]) )\n",
    "    return M3/len(y)\n",
    "\n",
    "def getM4(X, y):\n",
    "    M4 = np.zeros([d,d,d,d])\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M4 += y[iter]*(outer4I(X[iter]) - specOuterMat(np.outer(X[iter], X[iter])) + specOuterMat(np.identity(d)) )\n",
    "    return M4/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# M =  np.asarray(getM2(train_x, train_y))\n",
    "\n",
    "# check = np.zeros((d,d))\n",
    "# # check = np.zeros(d)\n",
    "# for i in range(k):\n",
    "#     check += v_gt[i]*m[1,i]*np.outer(W_gt[:,i], W_gt[:,i])\n",
    "# #     check += v_gt[i]*m[0,i]*W_gt[:,i]\n",
    "# print check\n",
    "# print np.linalg.norm(M-check)/np.linalg.norm(check)\n",
    "# print np.linalg.norm(M-check)/np.linalg.norm(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getP2V(V, X, y, k):\n",
    "    d = np.shape(X)[1]\n",
    "    P2V = np.zeros((d,k))\n",
    "    for i in range(len(X)):\n",
    "        P2V += y[i]*(np.dot(np.transpose([X[i]]), np.dot([X[i]], V) ) - V)\n",
    "    return P2V/np.shape(X)[0]\n",
    "        \n",
    "def getP2v(v, X, y):\n",
    "    d = np.shape(X)[1]\n",
    "    P2v = np.zeros((d,1))\n",
    "    for i in range(len(X)):\n",
    "        P2v += y[i]*(np.transpose([X[i]])*np.dot([X[i]], np.transpose([v]))  - np.transpose([v]))\n",
    "    return P2v/np.shape(X)[0]\n",
    "\n",
    "def topk(eigenV, k):\n",
    "    sortList = []\n",
    "    for i in range(2):\n",
    "        for j in range(k):\n",
    "            sortList.append([eigenV[i,j], (i,j)])\n",
    "    sortList.sort(reverse=True)\n",
    "    k1 = 0\n",
    "    k2 = 0\n",
    "    pi1 = {}\n",
    "    pi2 = {}\n",
    "    for i in range(k):\n",
    "        if sortList[i][1][0] == 0:\n",
    "            pi1[k1] = sortList[i][1][1]\n",
    "            k1 += 1\n",
    "        else:\n",
    "            pi2[k2] = sortList[i][1][1]\n",
    "            k2 += 1\n",
    "    return pi1, pi2, k1, k2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def powMeth( k, X, y):\n",
    "#     C = 3*np.linalg.norm(P2)\n",
    "    C = 3*np.linalg.norm(getM2(X,y))\n",
    "    T = 10000\n",
    "    d =  np.shape(X)[1]\n",
    "    V1 = np.random.normal(0.0, 0.1 , (d,k))\n",
    "    V2 = np.random.normal(0.0, 0.1 , (d,k))\n",
    "    \n",
    "    for i in range(T):\n",
    "        P2V1 = getP2V(V1, X, y, k)\n",
    "        P2V2 = getP2V(V2, X, y, k)\n",
    "        V1, temp = np.linalg.qr(C*V1 + P2V1)\n",
    "        V2, temp = np.linalg.qr(C*V2 - P2V2)\n",
    "    \n",
    "    eigenV = np.zeros((2,k))\n",
    "    for i in range(k):\n",
    "        eigenV[0,i] = abs( np.dot(np.transpose(V1[:,i]) , getP2v(V1[:,i], X, y) ) )\n",
    "    for i in range(k):\n",
    "        eigenV[1,i] = abs(np.dot(np.transpose(V2[:,i]) , getP2v(V2[:,i], X, y) ))\n",
    "    \n",
    "    pi1, pi2, k1, k2 = topk(eigenV, k)\n",
    "    \n",
    "    V1_new = np.zeros((d,k1))\n",
    "    V2_new = np.zeros((d,k2))\n",
    "    \n",
    "    for i in range(k1):\n",
    "        V1_new[:,i] = V1[:,pi1[i]]\n",
    "    for i in range(k2):\n",
    "        V2_new[:,i] = V2[:,pi2[i]]\n",
    "    V2_new, temp = np.linalg.qr(np.dot(np.identity(d)-np.dot(V1_new, np.transpose(V1_new)), V2_new))\n",
    "    return np.concatenate((V1_new, V2_new), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recmagsgn(V, U, X, y):\n",
    "    \n",
    "    ### l1 and l2 :: TODO\n",
    "    l1 = 1\n",
    "    l2 = 2\n",
    "    \n",
    "    d = np.shape(X)[1]\n",
    "    k = np.shape(U)[0]\n",
    "    divInd = int(len(X)/2)\n",
    "    \n",
    "    # Partition\n",
    "    X1 = X[:divInd]\n",
    "    y1 = y[:divInd]\n",
    "\n",
    "    X2 = X[divInd:]\n",
    "    y2 = y[divInd:]\n",
    "    \n",
    "    alpha = np.random.normal(0.0, 0.1 , d)\n",
    "    alpha = alpha/np.linalg.norm(alpha)\n",
    "    \n",
    "    ## TODO: Assuming l1=1 and l2=2 \n",
    "    Q1 = getM1(X1, y1)\n",
    "    Q2 = multLnr1(getM2(X2,y2), V).flatten()    \n",
    "    \n",
    "    Vu = []\n",
    "    UU = []\n",
    "\n",
    "    for ind in range(len(U)):\n",
    "        Vu.append(np.dot(V, U[ind]))\n",
    "        UU.append(np.dot( np.transpose([U[ind]]), [U[ind]] ).flatten())\n",
    "        \n",
    "    ## Estimating z\n",
    "    z_old = np.zeros(k)\n",
    "    z_new = np.zeros(k)\n",
    "    \n",
    "    T = 10000\n",
    "    for iterT in range(T):\n",
    "        for ind in range(k):\n",
    "            \n",
    "            mult_fact = np.zeros(d)\n",
    "            for j in range(k):\n",
    "                if j != ind:\n",
    "                    mult_fact += z_old[j]*Vu[j]\n",
    "            div_fact = np.dot(np.transpose(Vu[ind]), Vu[ind])\n",
    "            z_new[ind] = (np.dot(np.transpose(Q1), Vu[ind]) + np.dot(np.transpose(Vu[ind]), Q1) -\n",
    "                                 np.dot(np.transpose(Vu[ind]), mult_fact) - \n",
    "                                  np.dot(np.transpose(mult_fact), Vu[ind]) )/(2*div_fact)\n",
    "        z_old = z_new\n",
    "    \n",
    "    ## Estimating r\n",
    "    r = np.dot(np.linalg.inv(np.dot(UU, np.transpose(UU))), \n",
    "                                                   np.dot(UU,np.transpose([Q2]) ) )\n",
    "    v = np.sign(r*np.transpose([m[l2-1]]))\n",
    "    s = np.sign(v*np.transpose([z_new])*np.transpose([m[l2-1]]))\n",
    "\n",
    "    p = 1 ## p + 1 is degree of homogenity\n",
    "\n",
    "    w = []\n",
    "    for ind in range(k):\n",
    "        \n",
    "        w.append(s[ind]*np.math.pow(abs(z_new[ind]*np.linalg.norm(W_gt[:,ind])**(p+1)/(m[l1-1,ind]*np.math.pow( np.dot( [alpha],\n",
    "                    np.transpose([Vu[ind]]) ) ,l1-1))), 1.0/(p+1))*Vu[ind])\n",
    "    w = np.asarray(w)\n",
    "\n",
    "    return w,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensorInit(X, y):\n",
    "    divInd = int(len(X)/3)\n",
    "    \n",
    "    # Partition\n",
    "    X1 = X[:divInd]\n",
    "    y1 = y[:divInd]\n",
    "\n",
    "    X2 = X[divInd:2*divInd]\n",
    "    y2 = y[divInd:2*divInd]\n",
    "    \n",
    "    X3 = X[2*divInd:]\n",
    "    y3 = y[2*divInd:]\n",
    "    \n",
    "    ## P2 \n",
    "    ## Estimating P2 as M2 :: TODO\n",
    "    \n",
    "#     alpha = np.random.normal(0.0, 0.1 , d)\n",
    "#     P2 = getM2(X1,y1)\n",
    "#     print(np.linalg.norm(P2))\n",
    "\n",
    "    ## Power Method\n",
    "    V = powMeth(k, X1, y1)\n",
    "    \n",
    "    ## R3\n",
    "    R3 = multLnr2(getM3(X2, y2), V)\n",
    "    \n",
    "    #### KCL\n",
    "    R = matlab.double(R3.flatten().tolist())\n",
    "    eng = me.start_matlab()\n",
    "    U = eng.notf_frompy(R, 100, k)\n",
    "    eng.quit()\n",
    "    U = np.asarray(U)\n",
    "#     U = R3[0]\n",
    "\n",
    "    ## RecMagSign\n",
    "    return recmagsgn(V, U, X3, y3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorWeights = tensorInit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05224457,  0.23042807, -0.1175749 ,  0.16702047,  0.17657808,\n",
       "         -0.05569402,  0.22921827, -0.24372486, -0.26783597, -0.1631667 ,\n",
       "          0.24095493, -0.14074936, -0.00665661,  0.18733078, -0.29721189,\n",
       "         -0.06992958,  0.05275971, -0.24515093,  0.19360093, -0.0253632 ,\n",
       "         -0.01153394, -0.02308832, -0.03359161,  0.1317134 ,  0.0282088 ,\n",
       "          0.43556966, -0.49087687,  0.10835563,  0.19957721,  0.03709088],\n",
       "        [-0.00854335, -0.00675476, -0.01328092,  0.00847862, -0.02461828,\n",
       "          0.04737665,  0.03104783, -0.0013089 , -0.03697157,  0.0845654 ,\n",
       "         -0.03286064, -0.06832903, -0.01931108, -0.00615856, -0.0533922 ,\n",
       "         -0.06985842, -0.02460134, -0.0110278 ,  0.02377181,  0.0161817 ,\n",
       "          0.01770176, -0.04575858, -0.04052705,  0.0293219 , -0.00901915,\n",
       "         -0.01101586,  0.03833602, -0.0310053 , -0.01514332, -0.07892804],\n",
       "        [ 0.03880471,  0.00389723,  0.05263819, -0.07451318,  0.0189947 ,\n",
       "         -0.1221766 , -0.04026404,  0.15732558, -0.07528178,  0.02869064,\n",
       "         -0.02972565,  0.01523045,  0.11796956,  0.12022282,  0.08442866,\n",
       "         -0.08399843,  0.01572394, -0.07804318,  0.01248439,  0.02306801,\n",
       "          0.06493491, -0.28654723,  0.00834471, -0.00087704,  0.01711957,\n",
       "         -0.12144383, -0.00614885,  0.13291559, -0.0021153 , -0.04338085],\n",
       "        [-0.12820928,  0.11620345,  0.02064338, -0.08822518, -0.04508256,\n",
       "          0.05966289, -0.09475236, -0.16815402,  0.01512136,  0.05538013,\n",
       "         -0.00777579,  0.06378799, -0.12409075,  0.05387297, -0.30687092,\n",
       "          0.07632015,  0.30195428,  0.11926937, -0.02769494,  0.18954445,\n",
       "          0.05396923, -0.19022941,  0.0391189 ,  0.15390271,  0.0359735 ,\n",
       "          0.15993279, -0.34878417, -0.04107645, -0.09679666, -0.09139678],\n",
       "        [-0.09999504, -0.05886084,  0.197301  , -0.07857774,  0.02251643,\n",
       "          0.00344198, -0.13342564, -0.02668525, -0.12703247,  0.14957801,\n",
       "          0.16817272, -0.12171307,  0.0095377 ,  0.10084571,  0.04228161,\n",
       "          0.035465  , -0.0016092 ,  0.03317085,  0.04836779, -0.04029963,\n",
       "         -0.17878937, -0.05029984,  0.0976967 , -0.01208367,  0.1148838 ,\n",
       "         -0.23317074, -0.17722845, -0.05503414,  0.0760312 , -0.14849001]]),\n",
       " array([[-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def forwardprop(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Forward-propagation.\n",
    "    \"\"\"\n",
    "    h    = tf.square(tf.nn.relu((tf.matmul(X, w_1))))\n",
    "    yhat = tf.matmul(h, w_2)  # The \\varphi function\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  1 training loss:  1.15917\n",
      "Epoch =  2 training loss:  1.042\n",
      "Epoch =  3 training loss:  0.983065\n",
      "Epoch =  4 training loss:  0.916009\n",
      "Epoch =  5 training loss:  0.738114\n",
      "Epoch =  6 training loss:  0.316541\n",
      "Epoch =  7 training loss:  0.197126\n",
      "Epoch =  8 training loss:  0.257117\n",
      "Epoch =  9 training loss:  0.281459\n",
      "Epoch =  10 training loss:  0.286934\n",
      "Epoch =  11 training loss:  0.287955\n",
      "Epoch =  12 training loss:  0.28797\n",
      "Epoch =  13 training loss:  0.28777\n",
      "Epoch =  14 training loss:  0.287535\n",
      "Epoch =  15 training loss:  0.287338\n",
      "Epoch =  16 training loss:  0.287186\n",
      "Epoch =  17 training loss:  0.287071\n",
      "Epoch =  18 training loss:  0.286991\n",
      "Epoch =  19 training loss:  0.286938\n",
      "Epoch =  20 training loss:  0.286902\n",
      "Epoch =  21 training loss:  0.286874\n",
      "Epoch =  22 training loss:  0.286853\n",
      "Epoch =  23 training loss:  0.286839\n",
      "Epoch =  24 training loss:  0.286827\n",
      "Epoch =  25 training loss:  0.286819\n",
      "Epoch =  26 training loss:  0.286814\n",
      "Epoch =  27 training loss:  0.28681\n",
      "Epoch =  28 training loss:  0.286806\n",
      "Epoch =  29 training loss:  0.286805\n",
      "Epoch =  30 training loss:  0.286804\n",
      "Epoch =  31 training loss:  0.286803\n",
      "Epoch =  32 training loss:  0.286802\n",
      "Epoch =  33 training loss:  0.286801\n",
      "Epoch =  34 training loss:  0.286801\n",
      "Epoch =  35 training loss:  0.286801\n",
      "Epoch =  36 training loss:  0.286801\n",
      "Epoch =  37 training loss:  0.2868\n",
      "Epoch =  38 training loss:  0.2868\n",
      "Epoch =  39 training loss:  0.2868\n",
      "Epoch =  40 training loss:  0.2868\n",
      "Epoch =  41 training loss:  0.286801\n",
      "Epoch =  42 training loss:  0.2868\n",
      "Epoch =  43 training loss:  0.2868\n",
      "Epoch =  44 training loss:  0.2868\n",
      "Epoch =  45 training loss:  0.286799\n",
      "Epoch =  46 training loss:  0.2868\n",
      "Epoch =  47 training loss:  0.2868\n",
      "Epoch =  48 training loss:  0.2868\n",
      "Epoch =  49 training loss:  0.2868\n",
      "Epoch =  50 training loss:  0.2868\n",
      "Epoch =  51 training loss:  0.2868\n",
      "Epoch =  52 training loss:  0.2868\n",
      "Epoch =  53 training loss:  0.286801\n",
      "Epoch =  54 training loss:  0.2868\n",
      "Epoch =  55 training loss:  0.2868\n",
      "Epoch =  56 training loss:  0.2868\n",
      "Epoch =  57 training loss:  0.2868\n",
      "Epoch =  58 training loss:  0.2868\n",
      "Epoch =  59 training loss:  0.286799\n",
      "Epoch =  60 training loss:  0.286799\n",
      "Epoch =  61 training loss:  0.2868\n",
      "Epoch =  62 training loss:  0.2868\n",
      "Epoch =  63 training loss:  0.2868\n",
      "Epoch =  64 training loss:  0.2868\n",
      "Epoch =  65 training loss:  0.2868\n",
      "Epoch =  66 training loss:  0.2868\n",
      "Epoch =  67 training loss:  0.2868\n",
      "Epoch =  68 training loss:  0.2868\n",
      "Epoch =  69 training loss:  0.2868\n",
      "Epoch =  70 training loss:  0.2868\n",
      "Epoch =  71 training loss:  0.2868\n",
      "Epoch =  72 training loss:  0.2868\n",
      "Epoch =  73 training loss:  0.2868\n",
      "Epoch =  74 training loss:  0.2868\n",
      "Epoch =  75 training loss:  0.2868\n",
      "Epoch =  76 training loss:  0.2868\n",
      "Epoch =  77 training loss:  0.2868\n",
      "Epoch =  78 training loss:  0.2868\n",
      "Epoch =  79 training loss:  0.2868\n",
      "Epoch =  80 training loss:  0.2868\n",
      "Epoch =  81 training loss:  0.2868\n",
      "Epoch =  82 training loss:  0.2868\n",
      "Epoch =  83 training loss:  0.2868\n",
      "Epoch =  84 training loss:  0.2868\n",
      "Epoch =  85 training loss:  0.2868\n",
      "Epoch =  86 training loss:  0.2868\n",
      "Epoch =  87 training loss:  0.2868\n",
      "Epoch =  88 training loss:  0.2868\n",
      "Epoch =  89 training loss:  0.2868\n",
      "Epoch =  90 training loss:  0.2868\n",
      "Epoch =  91 training loss:  0.2868\n",
      "Epoch =  92 training loss:  0.2868\n",
      "Epoch =  93 training loss:  0.2868\n",
      "Epoch =  94 training loss:  0.2868\n",
      "Epoch =  95 training loss:  0.2868\n",
      "Epoch =  96 training loss:  0.2868\n",
      "Epoch =  97 training loss:  0.2868\n",
      "Epoch =  98 training loss:  0.2868\n",
      "Epoch =  99 training loss:  0.2868\n",
      "Epoch =  100 training loss:  0.2868\n",
      "Epoch =  101 training loss:  0.2868\n",
      "Epoch =  102 training loss:  0.2868\n",
      "Epoch =  103 training loss:  0.2868\n",
      "Epoch =  104 training loss:  0.2868\n",
      "Epoch =  105 training loss:  0.2868\n",
      "Epoch =  106 training loss:  0.2868\n",
      "Epoch =  107 training loss:  0.2868\n",
      "Epoch =  108 training loss:  0.2868\n",
      "Epoch =  109 training loss:  0.2868\n",
      "Epoch =  110 training loss:  0.2868\n",
      "Epoch =  111 training loss:  0.2868\n",
      "Epoch =  112 training loss:  0.2868\n",
      "Epoch =  113 training loss:  0.2868\n",
      "Epoch =  114 training loss:  0.286799\n",
      "Epoch =  115 training loss:  0.286801\n",
      "Epoch =  116 training loss:  0.2868\n",
      "Epoch =  117 training loss:  0.2868\n",
      "Epoch =  118 training loss:  0.2868\n",
      "Epoch =  119 training loss:  0.2868\n",
      "Epoch =  120 training loss:  0.2868\n",
      "Epoch =  121 training loss:  0.2868\n",
      "Epoch =  122 training loss:  0.2868\n",
      "Epoch =  123 training loss:  0.2868\n",
      "Epoch =  124 training loss:  0.2868\n",
      "Epoch =  125 training loss:  0.2868\n",
      "Epoch =  126 training loss:  0.2868\n",
      "Epoch =  127 training loss:  0.2868\n",
      "Epoch =  128 training loss:  0.2868\n",
      "Epoch =  129 training loss:  0.2868\n",
      "Epoch =  130 training loss:  0.2868\n",
      "Epoch =  131 training loss:  0.2868\n",
      "Epoch =  132 training loss:  0.2868\n",
      "Epoch =  133 training loss:  0.286801\n",
      "Epoch =  134 training loss:  0.2868\n",
      "Epoch =  135 training loss:  0.2868\n",
      "Epoch =  136 training loss:  0.2868\n",
      "Epoch =  137 training loss:  0.2868\n",
      "Epoch =  138 training loss:  0.2868\n",
      "Epoch =  139 training loss:  0.2868\n",
      "Epoch =  140 training loss:  0.2868\n",
      "Epoch =  141 training loss:  0.2868\n",
      "Epoch =  142 training loss:  0.2868\n",
      "Epoch =  143 training loss:  0.2868\n",
      "Epoch =  144 training loss:  0.2868\n",
      "Epoch =  145 training loss:  0.2868\n",
      "Epoch =  146 training loss:  0.2868\n",
      "Epoch =  147 training loss:  0.2868\n",
      "Epoch =  148 training loss:  0.2868\n",
      "Epoch =  149 training loss:  0.286801\n",
      "Epoch =  150 training loss:  0.2868\n",
      "Epoch =  151 training loss:  0.2868\n",
      "Epoch =  152 training loss:  0.2868\n",
      "Epoch =  153 training loss:  0.2868\n",
      "Epoch =  154 training loss:  0.2868\n",
      "Epoch =  155 training loss:  0.2868\n",
      "Epoch =  156 training loss:  0.2868\n",
      "Epoch =  157 training loss:  0.2868\n",
      "Epoch =  158 training loss:  0.2868\n",
      "Epoch =  159 training loss:  0.2868\n",
      "Epoch =  160 training loss:  0.2868\n",
      "Epoch =  161 training loss:  0.2868\n",
      "Epoch =  162 training loss:  0.2868\n",
      "Epoch =  163 training loss:  0.2868\n",
      "Epoch =  164 training loss:  0.2868\n",
      "Epoch =  165 training loss:  0.286799\n",
      "Epoch =  166 training loss:  0.2868\n",
      "Epoch =  167 training loss:  0.2868\n",
      "Epoch =  168 training loss:  0.2868\n",
      "Epoch =  169 training loss:  0.2868\n",
      "Epoch =  170 training loss:  0.2868\n",
      "Epoch =  171 training loss:  0.2868\n",
      "Epoch =  172 training loss:  0.2868\n",
      "Epoch =  173 training loss:  0.286801\n",
      "Epoch =  174 training loss:  0.2868\n",
      "Epoch =  175 training loss:  0.2868\n",
      "Epoch =  176 training loss:  0.2868\n",
      "Epoch =  177 training loss:  0.2868\n",
      "Epoch =  178 training loss:  0.286799\n",
      "Epoch =  179 training loss:  0.2868\n",
      "Epoch =  180 training loss:  0.286801\n",
      "Epoch =  181 training loss:  0.2868\n",
      "Epoch =  182 training loss:  0.2868\n",
      "Epoch =  183 training loss:  0.2868\n",
      "Epoch =  184 training loss:  0.2868\n",
      "Epoch =  185 training loss:  0.2868\n",
      "Epoch =  186 training loss:  0.2868\n",
      "Epoch =  187 training loss:  0.2868\n",
      "Epoch =  188 training loss:  0.2868\n",
      "Epoch =  189 training loss:  0.2868\n",
      "Epoch =  190 training loss:  0.2868\n",
      "Epoch =  191 training loss:  0.2868\n",
      "Epoch =  192 training loss:  0.286799\n",
      "Epoch =  193 training loss:  0.2868\n",
      "Epoch =  194 training loss:  0.2868\n",
      "Epoch =  195 training loss:  0.2868\n",
      "Epoch =  196 training loss:  0.2868\n",
      "Epoch =  197 training loss:  0.2868\n",
      "Epoch =  198 training loss:  0.2868\n",
      "Epoch =  199 training loss:  0.286799\n",
      "Epoch =  200 training loss:  0.286799\n",
      "Epoch =  201 training loss:  0.2868\n",
      "Epoch =  202 training loss:  0.286799\n",
      "Epoch =  203 training loss:  0.2868\n",
      "Epoch =  204 training loss:  0.2868\n",
      "Epoch =  205 training loss:  0.2868\n",
      "Epoch =  206 training loss:  0.286801\n",
      "Epoch =  207 training loss:  0.2868\n",
      "Epoch =  208 training loss:  0.2868\n",
      "Epoch =  209 training loss:  0.2868\n",
      "Epoch =  210 training loss:  0.2868\n",
      "Epoch =  211 training loss:  0.2868\n",
      "Epoch =  212 training loss:  0.286799\n",
      "Epoch =  213 training loss:  0.2868\n",
      "Epoch =  214 training loss:  0.2868\n",
      "Epoch =  215 training loss:  0.2868\n",
      "Epoch =  216 training loss:  0.2868\n",
      "Epoch =  217 training loss:  0.2868\n",
      "Epoch =  218 training loss:  0.2868\n",
      "Epoch =  219 training loss:  0.2868\n",
      "Epoch =  220 training loss:  0.2868\n",
      "Epoch =  221 training loss:  0.2868\n",
      "Epoch =  222 training loss:  0.2868\n",
      "Epoch =  223 training loss:  0.2868\n",
      "Epoch =  224 training loss:  0.286799\n",
      "Epoch =  225 training loss:  0.286799\n",
      "Epoch =  226 training loss:  0.2868\n",
      "Epoch =  227 training loss:  0.2868\n",
      "Epoch =  228 training loss:  0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  229 training loss:  0.2868\n",
      "Epoch =  230 training loss:  0.2868\n",
      "Epoch =  231 training loss:  0.2868\n",
      "Epoch =  232 training loss:  0.2868\n",
      "Epoch =  233 training loss:  0.2868\n",
      "Epoch =  234 training loss:  0.2868\n",
      "Epoch =  235 training loss:  0.2868\n",
      "Epoch =  236 training loss:  0.2868\n",
      "Epoch =  237 training loss:  0.2868\n",
      "Epoch =  238 training loss:  0.2868\n",
      "Epoch =  239 training loss:  0.286799\n",
      "Epoch =  240 training loss:  0.2868\n",
      "Epoch =  241 training loss:  0.2868\n",
      "Epoch =  242 training loss:  0.2868\n",
      "Epoch =  243 training loss:  0.2868\n",
      "Epoch =  244 training loss:  0.2868\n",
      "Epoch =  245 training loss:  0.2868\n",
      "Epoch =  246 training loss:  0.2868\n",
      "Epoch =  247 training loss:  0.2868\n",
      "Epoch =  248 training loss:  0.2868\n",
      "Epoch =  249 training loss:  0.2868\n",
      "Epoch =  250 training loss:  0.2868\n",
      "Epoch =  251 training loss:  0.2868\n",
      "Epoch =  252 training loss:  0.2868\n",
      "Epoch =  253 training loss:  0.2868\n",
      "Epoch =  254 training loss:  0.2868\n",
      "Epoch =  255 training loss:  0.2868\n",
      "Epoch =  256 training loss:  0.2868\n",
      "Epoch =  257 training loss:  0.2868\n",
      "Epoch =  258 training loss:  0.2868\n",
      "Epoch =  259 training loss:  0.2868\n",
      "Epoch =  260 training loss:  0.2868\n",
      "Epoch =  261 training loss:  0.2868\n",
      "Epoch =  262 training loss:  0.2868\n",
      "Epoch =  263 training loss:  0.2868\n",
      "Epoch =  264 training loss:  0.2868\n",
      "Epoch =  265 training loss:  0.286801\n",
      "Epoch =  266 training loss:  0.2868\n",
      "Epoch =  267 training loss:  0.2868\n",
      "Epoch =  268 training loss:  0.2868\n",
      "Epoch =  269 training loss:  0.2868\n",
      "Epoch =  270 training loss:  0.2868\n",
      "Epoch =  271 training loss:  0.2868\n",
      "Epoch =  272 training loss:  0.2868\n",
      "Epoch =  273 training loss:  0.286799\n",
      "Epoch =  274 training loss:  0.2868\n",
      "Epoch =  275 training loss:  0.2868\n",
      "Epoch =  276 training loss:  0.2868\n",
      "Epoch =  277 training loss:  0.2868\n",
      "Epoch =  278 training loss:  0.2868\n",
      "Epoch =  279 training loss:  0.286801\n",
      "Epoch =  280 training loss:  0.2868\n",
      "Epoch =  281 training loss:  0.2868\n",
      "Epoch =  282 training loss:  0.2868\n",
      "Epoch =  283 training loss:  0.2868\n",
      "Epoch =  284 training loss:  0.2868\n",
      "Epoch =  285 training loss:  0.2868\n",
      "Epoch =  286 training loss:  0.2868\n",
      "Epoch =  287 training loss:  0.286799\n",
      "Epoch =  288 training loss:  0.2868\n",
      "Epoch =  289 training loss:  0.2868\n",
      "Epoch =  290 training loss:  0.2868\n",
      "Epoch =  291 training loss:  0.2868\n",
      "Epoch =  292 training loss:  0.2868\n",
      "Epoch =  293 training loss:  0.2868\n",
      "Epoch =  294 training loss:  0.2868\n",
      "Epoch =  295 training loss:  0.2868\n",
      "Epoch =  296 training loss:  0.2868\n",
      "Epoch =  297 training loss:  0.2868\n",
      "Epoch =  298 training loss:  0.2868\n",
      "Epoch =  299 training loss:  0.2868\n",
      "Epoch =  300 training loss:  0.2868\n",
      "Epoch =  301 training loss:  0.2868\n",
      "Epoch =  302 training loss:  0.2868\n",
      "Epoch =  303 training loss:  0.286799\n",
      "Epoch =  304 training loss:  0.2868\n",
      "Epoch =  305 training loss:  0.2868\n",
      "Epoch =  306 training loss:  0.2868\n",
      "Epoch =  307 training loss:  0.2868\n",
      "Epoch =  308 training loss:  0.2868\n",
      "Epoch =  309 training loss:  0.2868\n",
      "Epoch =  310 training loss:  0.2868\n",
      "Epoch =  311 training loss:  0.2868\n",
      "Epoch =  312 training loss:  0.2868\n",
      "Epoch =  313 training loss:  0.2868\n",
      "Epoch =  314 training loss:  0.2868\n",
      "Epoch =  315 training loss:  0.2868\n",
      "Epoch =  316 training loss:  0.286799\n",
      "Epoch =  317 training loss:  0.2868\n",
      "Epoch =  318 training loss:  0.2868\n",
      "Epoch =  319 training loss:  0.2868\n",
      "Epoch =  320 training loss:  0.2868\n",
      "Epoch =  321 training loss:  0.2868\n",
      "Epoch =  322 training loss:  0.2868\n",
      "Epoch =  323 training loss:  0.2868\n",
      "Epoch =  324 training loss:  0.2868\n",
      "Epoch =  325 training loss:  0.2868\n",
      "Epoch =  326 training loss:  0.2868\n",
      "Epoch =  327 training loss:  0.2868\n",
      "Epoch =  328 training loss:  0.2868\n",
      "Epoch =  329 training loss:  0.2868\n",
      "Epoch =  330 training loss:  0.2868\n",
      "Epoch =  331 training loss:  0.2868\n",
      "Epoch =  332 training loss:  0.2868\n",
      "Epoch =  333 training loss:  0.2868\n",
      "Epoch =  334 training loss:  0.2868\n",
      "Epoch =  335 training loss:  0.2868\n",
      "Epoch =  336 training loss:  0.2868\n",
      "Epoch =  337 training loss:  0.2868\n",
      "Epoch =  338 training loss:  0.2868\n",
      "Epoch =  339 training loss:  0.2868\n",
      "Epoch =  340 training loss:  0.2868\n",
      "Epoch =  341 training loss:  0.2868\n",
      "Epoch =  342 training loss:  0.2868\n",
      "Epoch =  343 training loss:  0.2868\n",
      "Epoch =  344 training loss:  0.286799\n",
      "Epoch =  345 training loss:  0.2868\n",
      "Epoch =  346 training loss:  0.2868\n",
      "Epoch =  347 training loss:  0.2868\n",
      "Epoch =  348 training loss:  0.286801\n",
      "Epoch =  349 training loss:  0.2868\n",
      "Epoch =  350 training loss:  0.2868\n",
      "Epoch =  351 training loss:  0.286799\n",
      "Epoch =  352 training loss:  0.2868\n",
      "Epoch =  353 training loss:  0.2868\n",
      "Epoch =  354 training loss:  0.2868\n",
      "Epoch =  355 training loss:  0.2868\n",
      "Epoch =  356 training loss:  0.2868\n",
      "Epoch =  357 training loss:  0.286799\n",
      "Epoch =  358 training loss:  0.2868\n",
      "Epoch =  359 training loss:  0.2868\n",
      "Epoch =  360 training loss:  0.2868\n",
      "Epoch =  361 training loss:  0.2868\n",
      "Epoch =  362 training loss:  0.2868\n",
      "Epoch =  363 training loss:  0.2868\n",
      "Epoch =  364 training loss:  0.2868\n",
      "Epoch =  365 training loss:  0.2868\n",
      "Epoch =  366 training loss:  0.2868\n",
      "Epoch =  367 training loss:  0.2868\n",
      "Epoch =  368 training loss:  0.2868\n",
      "Epoch =  369 training loss:  0.2868\n",
      "Epoch =  370 training loss:  0.2868\n",
      "Epoch =  371 training loss:  0.2868\n",
      "Epoch =  372 training loss:  0.286799\n",
      "Epoch =  373 training loss:  0.2868\n",
      "Epoch =  374 training loss:  0.2868\n",
      "Epoch =  375 training loss:  0.286799\n",
      "Epoch =  376 training loss:  0.2868\n",
      "Epoch =  377 training loss:  0.2868\n",
      "Epoch =  378 training loss:  0.2868\n",
      "Epoch =  379 training loss:  0.286799\n",
      "Epoch =  380 training loss:  0.2868\n",
      "Epoch =  381 training loss:  0.2868\n",
      "Epoch =  382 training loss:  0.2868\n",
      "Epoch =  383 training loss:  0.286801\n",
      "Epoch =  384 training loss:  0.2868\n",
      "Epoch =  385 training loss:  0.2868\n",
      "Epoch =  386 training loss:  0.2868\n",
      "Epoch =  387 training loss:  0.2868\n",
      "Epoch =  388 training loss:  0.2868\n",
      "Epoch =  389 training loss:  0.2868\n",
      "Epoch =  390 training loss:  0.2868\n",
      "Epoch =  391 training loss:  0.2868\n",
      "Epoch =  392 training loss:  0.2868\n",
      "Epoch =  393 training loss:  0.286799\n",
      "Epoch =  394 training loss:  0.2868\n",
      "Epoch =  395 training loss:  0.2868\n",
      "Epoch =  396 training loss:  0.2868\n",
      "Epoch =  397 training loss:  0.2868\n",
      "Epoch =  398 training loss:  0.2868\n",
      "Epoch =  399 training loss:  0.2868\n",
      "Epoch =  400 training loss:  0.2868\n",
      "Epoch =  401 training loss:  0.2868\n",
      "Epoch =  402 training loss:  0.2868\n",
      "Epoch =  403 training loss:  0.286799\n",
      "Epoch =  404 training loss:  0.2868\n",
      "Epoch =  405 training loss:  0.2868\n",
      "Epoch =  406 training loss:  0.286799\n",
      "Epoch =  407 training loss:  0.2868\n",
      "Epoch =  408 training loss:  0.2868\n",
      "Epoch =  409 training loss:  0.2868\n",
      "Epoch =  410 training loss:  0.2868\n",
      "Epoch =  411 training loss:  0.2868\n",
      "Epoch =  412 training loss:  0.286799\n",
      "Epoch =  413 training loss:  0.2868\n",
      "Epoch =  414 training loss:  0.2868\n",
      "Epoch =  415 training loss:  0.2868\n",
      "Epoch =  416 training loss:  0.2868\n",
      "Epoch =  417 training loss:  0.2868\n",
      "Epoch =  418 training loss:  0.2868\n",
      "Epoch =  419 training loss:  0.2868\n",
      "Epoch =  420 training loss:  0.286801\n",
      "Epoch =  421 training loss:  0.2868\n",
      "Epoch =  422 training loss:  0.2868\n",
      "Epoch =  423 training loss:  0.2868\n",
      "Epoch =  424 training loss:  0.2868\n",
      "Epoch =  425 training loss:  0.2868\n",
      "Epoch =  426 training loss:  0.2868\n",
      "Epoch =  427 training loss:  0.2868\n",
      "Epoch =  428 training loss:  0.2868\n",
      "Epoch =  429 training loss:  0.286799\n",
      "Epoch =  430 training loss:  0.2868\n",
      "Epoch =  431 training loss:  0.2868\n",
      "Epoch =  432 training loss:  0.2868\n",
      "Epoch =  433 training loss:  0.2868\n",
      "Epoch =  434 training loss:  0.286801\n",
      "Epoch =  435 training loss:  0.2868\n",
      "Epoch =  436 training loss:  0.2868\n",
      "Epoch =  437 training loss:  0.2868\n",
      "Epoch =  438 training loss:  0.286799\n",
      "Epoch =  439 training loss:  0.2868\n",
      "Epoch =  440 training loss:  0.2868\n",
      "Epoch =  441 training loss:  0.2868\n",
      "Epoch =  442 training loss:  0.2868\n",
      "Epoch =  443 training loss:  0.2868\n",
      "Epoch =  444 training loss:  0.2868\n",
      "Epoch =  445 training loss:  0.286799\n",
      "Epoch =  446 training loss:  0.2868\n",
      "Epoch =  447 training loss:  0.2868\n",
      "Epoch =  448 training loss:  0.286799\n",
      "Epoch =  449 training loss:  0.2868\n",
      "Epoch =  450 training loss:  0.2868\n",
      "Epoch =  451 training loss:  0.2868\n",
      "Epoch =  452 training loss:  0.2868\n",
      "Epoch =  453 training loss:  0.2868\n",
      "Epoch =  454 training loss:  0.286799\n",
      "Epoch =  455 training loss:  0.2868\n",
      "Epoch =  456 training loss:  0.286801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  457 training loss:  0.2868\n",
      "Epoch =  458 training loss:  0.2868\n",
      "Epoch =  459 training loss:  0.2868\n",
      "Epoch =  460 training loss:  0.2868\n",
      "Epoch =  461 training loss:  0.2868\n",
      "Epoch =  462 training loss:  0.286799\n",
      "Epoch =  463 training loss:  0.2868\n",
      "Epoch =  464 training loss:  0.2868\n",
      "Epoch =  465 training loss:  0.2868\n",
      "Epoch =  466 training loss:  0.2868\n",
      "Epoch =  467 training loss:  0.2868\n",
      "Epoch =  468 training loss:  0.286801\n",
      "Epoch =  469 training loss:  0.2868\n",
      "Epoch =  470 training loss:  0.2868\n",
      "Epoch =  471 training loss:  0.2868\n",
      "Epoch =  472 training loss:  0.2868\n",
      "Epoch =  473 training loss:  0.2868\n",
      "Epoch =  474 training loss:  0.2868\n",
      "Epoch =  475 training loss:  0.286801\n",
      "Epoch =  476 training loss:  0.2868\n",
      "Epoch =  477 training loss:  0.2868\n",
      "Epoch =  478 training loss:  0.286801\n",
      "Epoch =  479 training loss:  0.2868\n",
      "Epoch =  480 training loss:  0.286799\n",
      "Epoch =  481 training loss:  0.2868\n",
      "Epoch =  482 training loss:  0.2868\n",
      "Epoch =  483 training loss:  0.2868\n",
      "Epoch =  484 training loss:  0.2868\n",
      "Epoch =  485 training loss:  0.286799\n",
      "Epoch =  486 training loss:  0.2868\n",
      "Epoch =  487 training loss:  0.286801\n",
      "Epoch =  488 training loss:  0.2868\n",
      "Epoch =  489 training loss:  0.2868\n",
      "Epoch =  490 training loss:  0.2868\n",
      "Epoch =  491 training loss:  0.286801\n",
      "Epoch =  492 training loss:  0.2868\n",
      "Epoch =  493 training loss:  0.286799\n",
      "Epoch =  494 training loss:  0.2868\n",
      "Epoch =  495 training loss:  0.2868\n",
      "Epoch =  496 training loss:  0.286801\n",
      "Epoch =  497 training loss:  0.2868\n",
      "Epoch =  498 training loss:  0.2868\n",
      "Epoch =  499 training loss:  0.2868\n",
      "Epoch =  500 training loss:  0.2868\n",
      "Epoch =  501 training loss:  0.2868\n",
      "Epoch =  502 training loss:  0.2868\n",
      "Epoch =  503 training loss:  0.2868\n",
      "Epoch =  504 training loss:  0.2868\n",
      "Epoch =  505 training loss:  0.286799\n",
      "Epoch =  506 training loss:  0.2868\n",
      "Epoch =  507 training loss:  0.2868\n",
      "Epoch =  508 training loss:  0.2868\n",
      "Epoch =  509 training loss:  0.2868\n",
      "Epoch =  510 training loss:  0.286799\n",
      "Epoch =  511 training loss:  0.2868\n",
      "Epoch =  512 training loss:  0.2868\n",
      "Epoch =  513 training loss:  0.286801\n",
      "Epoch =  514 training loss:  0.2868\n",
      "Epoch =  515 training loss:  0.2868\n",
      "Epoch =  516 training loss:  0.2868\n",
      "Epoch =  517 training loss:  0.2868\n",
      "Epoch =  518 training loss:  0.286799\n",
      "Epoch =  519 training loss:  0.2868\n",
      "Epoch =  520 training loss:  0.2868\n",
      "Epoch =  521 training loss:  0.2868\n",
      "Epoch =  522 training loss:  0.2868\n",
      "Epoch =  523 training loss:  0.2868\n",
      "Epoch =  524 training loss:  0.2868\n",
      "Epoch =  525 training loss:  0.2868\n",
      "Epoch =  526 training loss:  0.2868\n",
      "Epoch =  527 training loss:  0.2868\n",
      "Epoch =  528 training loss:  0.286799\n",
      "Epoch =  529 training loss:  0.2868\n",
      "Epoch =  530 training loss:  0.2868\n",
      "Epoch =  531 training loss:  0.2868\n",
      "Epoch =  532 training loss:  0.2868\n",
      "Epoch =  533 training loss:  0.2868\n",
      "Epoch =  534 training loss:  0.2868\n",
      "Epoch =  535 training loss:  0.2868\n",
      "Epoch =  536 training loss:  0.2868\n",
      "Epoch =  537 training loss:  0.2868\n",
      "Epoch =  538 training loss:  0.2868\n",
      "Epoch =  539 training loss:  0.2868\n",
      "Epoch =  540 training loss:  0.2868\n",
      "Epoch =  541 training loss:  0.2868\n",
      "Epoch =  542 training loss:  0.286799\n",
      "Epoch =  543 training loss:  0.2868\n",
      "Epoch =  544 training loss:  0.2868\n",
      "Epoch =  545 training loss:  0.2868\n",
      "Epoch =  546 training loss:  0.2868\n",
      "Epoch =  547 training loss:  0.2868\n",
      "Epoch =  548 training loss:  0.2868\n",
      "Epoch =  549 training loss:  0.2868\n",
      "Epoch =  550 training loss:  0.2868\n",
      "Epoch =  551 training loss:  0.2868\n",
      "Epoch =  552 training loss:  0.2868\n",
      "Epoch =  553 training loss:  0.2868\n",
      "Epoch =  554 training loss:  0.2868\n",
      "Epoch =  555 training loss:  0.2868\n",
      "Epoch =  556 training loss:  0.2868\n",
      "Epoch =  557 training loss:  0.2868\n",
      "Epoch =  558 training loss:  0.2868\n",
      "Epoch =  559 training loss:  0.286801\n",
      "Epoch =  560 training loss:  0.286801\n",
      "Epoch =  561 training loss:  0.2868\n",
      "Epoch =  562 training loss:  0.2868\n",
      "Epoch =  563 training loss:  0.286801\n",
      "Epoch =  564 training loss:  0.2868\n",
      "Epoch =  565 training loss:  0.2868\n",
      "Epoch =  566 training loss:  0.2868\n",
      "Epoch =  567 training loss:  0.2868\n",
      "Epoch =  568 training loss:  0.2868\n",
      "Epoch =  569 training loss:  0.2868\n",
      "Epoch =  570 training loss:  0.2868\n",
      "Epoch =  571 training loss:  0.2868\n",
      "Epoch =  572 training loss:  0.2868\n",
      "Epoch =  573 training loss:  0.2868\n",
      "Epoch =  574 training loss:  0.286801\n",
      "Epoch =  575 training loss:  0.286799\n",
      "Epoch =  576 training loss:  0.2868\n",
      "Epoch =  577 training loss:  0.2868\n",
      "Epoch =  578 training loss:  0.2868\n",
      "Epoch =  579 training loss:  0.2868\n",
      "Epoch =  580 training loss:  0.2868\n",
      "Epoch =  581 training loss:  0.286799\n",
      "Epoch =  582 training loss:  0.2868\n",
      "Epoch =  583 training loss:  0.2868\n",
      "Epoch =  584 training loss:  0.286801\n",
      "Epoch =  585 training loss:  0.2868\n",
      "Epoch =  586 training loss:  0.2868\n",
      "Epoch =  587 training loss:  0.2868\n",
      "Epoch =  588 training loss:  0.2868\n",
      "Epoch =  589 training loss:  0.286799\n",
      "Epoch =  590 training loss:  0.2868\n",
      "Epoch =  591 training loss:  0.286799\n",
      "Epoch =  592 training loss:  0.2868\n",
      "Epoch =  593 training loss:  0.2868\n",
      "Epoch =  594 training loss:  0.2868\n",
      "Epoch =  595 training loss:  0.2868\n",
      "Epoch =  596 training loss:  0.2868\n",
      "Epoch =  597 training loss:  0.2868\n",
      "Epoch =  598 training loss:  0.2868\n",
      "Epoch =  599 training loss:  0.286801\n",
      "Epoch =  600 training loss:  0.2868\n",
      "Epoch =  601 training loss:  0.2868\n",
      "Epoch =  602 training loss:  0.286799\n",
      "Epoch =  603 training loss:  0.2868\n",
      "Epoch =  604 training loss:  0.2868\n",
      "Epoch =  605 training loss:  0.2868\n",
      "Epoch =  606 training loss:  0.2868\n",
      "Epoch =  607 training loss:  0.2868\n",
      "Epoch =  608 training loss:  0.2868\n",
      "Epoch =  609 training loss:  0.2868\n",
      "Epoch =  610 training loss:  0.2868\n",
      "Epoch =  611 training loss:  0.2868\n",
      "Epoch =  612 training loss:  0.2868\n",
      "Epoch =  613 training loss:  0.286799\n",
      "Epoch =  614 training loss:  0.2868\n",
      "Epoch =  615 training loss:  0.2868\n",
      "Epoch =  616 training loss:  0.2868\n",
      "Epoch =  617 training loss:  0.2868\n",
      "Epoch =  618 training loss:  0.2868\n",
      "Epoch =  619 training loss:  0.2868\n",
      "Epoch =  620 training loss:  0.2868\n",
      "Epoch =  621 training loss:  0.2868\n",
      "Epoch =  622 training loss:  0.2868\n",
      "Epoch =  623 training loss:  0.2868\n",
      "Epoch =  624 training loss:  0.2868\n",
      "Epoch =  625 training loss:  0.286799\n",
      "Epoch =  626 training loss:  0.2868\n",
      "Epoch =  627 training loss:  0.2868\n",
      "Epoch =  628 training loss:  0.2868\n",
      "Epoch =  629 training loss:  0.2868\n",
      "Epoch =  630 training loss:  0.2868\n",
      "Epoch =  631 training loss:  0.2868\n",
      "Epoch =  632 training loss:  0.2868\n",
      "Epoch =  633 training loss:  0.2868\n",
      "Epoch =  634 training loss:  0.286799\n",
      "Epoch =  635 training loss:  0.2868\n",
      "Epoch =  636 training loss:  0.2868\n",
      "Epoch =  637 training loss:  0.286799\n",
      "Epoch =  638 training loss:  0.2868\n",
      "Epoch =  639 training loss:  0.2868\n",
      "Epoch =  640 training loss:  0.2868\n",
      "Epoch =  641 training loss:  0.2868\n",
      "Epoch =  642 training loss:  0.2868\n",
      "Epoch =  643 training loss:  0.2868\n",
      "Epoch =  644 training loss:  0.2868\n",
      "Epoch =  645 training loss:  0.2868\n",
      "Epoch =  646 training loss:  0.286799\n",
      "Epoch =  647 training loss:  0.2868\n",
      "Epoch =  648 training loss:  0.2868\n",
      "Epoch =  649 training loss:  0.2868\n",
      "Epoch =  650 training loss:  0.286799\n",
      "Epoch =  651 training loss:  0.2868\n",
      "Epoch =  652 training loss:  0.286799\n",
      "Epoch =  653 training loss:  0.2868\n",
      "Epoch =  654 training loss:  0.2868\n",
      "Epoch =  655 training loss:  0.2868\n",
      "Epoch =  656 training loss:  0.2868\n",
      "Epoch =  657 training loss:  0.2868\n",
      "Epoch =  658 training loss:  0.2868\n",
      "Epoch =  659 training loss:  0.286799\n",
      "Epoch =  660 training loss:  0.2868\n",
      "Epoch =  661 training loss:  0.2868\n",
      "Epoch =  662 training loss:  0.2868\n",
      "Epoch =  663 training loss:  0.2868\n",
      "Epoch =  664 training loss:  0.286799\n",
      "Epoch =  665 training loss:  0.2868\n",
      "Epoch =  666 training loss:  0.2868\n",
      "Epoch =  667 training loss:  0.2868\n",
      "Epoch =  668 training loss:  0.2868\n",
      "Epoch =  669 training loss:  0.2868\n",
      "Epoch =  670 training loss:  0.2868\n",
      "Epoch =  671 training loss:  0.2868\n",
      "Epoch =  672 training loss:  0.2868\n",
      "Epoch =  673 training loss:  0.2868\n",
      "Epoch =  674 training loss:  0.2868\n",
      "Epoch =  675 training loss:  0.2868\n",
      "Epoch =  676 training loss:  0.2868\n",
      "Epoch =  677 training loss:  0.2868\n",
      "Epoch =  678 training loss:  0.2868\n",
      "Epoch =  679 training loss:  0.2868\n",
      "Epoch =  680 training loss:  0.2868\n",
      "Epoch =  681 training loss:  0.2868\n",
      "Epoch =  682 training loss:  0.2868\n",
      "Epoch =  683 training loss:  0.2868\n",
      "Epoch =  684 training loss:  0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  685 training loss:  0.2868\n",
      "Epoch =  686 training loss:  0.2868\n",
      "Epoch =  687 training loss:  0.2868\n",
      "Epoch =  688 training loss:  0.286799\n",
      "Epoch =  689 training loss:  0.2868\n",
      "Epoch =  690 training loss:  0.2868\n",
      "Epoch =  691 training loss:  0.286799\n",
      "Epoch =  692 training loss:  0.2868\n",
      "Epoch =  693 training loss:  0.2868\n",
      "Epoch =  694 training loss:  0.2868\n",
      "Epoch =  695 training loss:  0.2868\n",
      "Epoch =  696 training loss:  0.286801\n",
      "Epoch =  697 training loss:  0.2868\n",
      "Epoch =  698 training loss:  0.2868\n",
      "Epoch =  699 training loss:  0.2868\n",
      "Epoch =  700 training loss:  0.2868\n",
      "Epoch =  701 training loss:  0.2868\n",
      "Epoch =  702 training loss:  0.2868\n",
      "Epoch =  703 training loss:  0.286801\n",
      "Epoch =  704 training loss:  0.2868\n",
      "Epoch =  705 training loss:  0.2868\n",
      "Epoch =  706 training loss:  0.2868\n",
      "Epoch =  707 training loss:  0.2868\n",
      "Epoch =  708 training loss:  0.2868\n",
      "Epoch =  709 training loss:  0.2868\n",
      "Epoch =  710 training loss:  0.2868\n",
      "Epoch =  711 training loss:  0.2868\n",
      "Epoch =  712 training loss:  0.2868\n",
      "Epoch =  713 training loss:  0.286799\n",
      "Epoch =  714 training loss:  0.2868\n",
      "Epoch =  715 training loss:  0.2868\n",
      "Epoch =  716 training loss:  0.2868\n",
      "Epoch =  717 training loss:  0.286799\n",
      "Epoch =  718 training loss:  0.2868\n",
      "Epoch =  719 training loss:  0.2868\n",
      "Epoch =  720 training loss:  0.2868\n",
      "Epoch =  721 training loss:  0.2868\n",
      "Epoch =  722 training loss:  0.2868\n",
      "Epoch =  723 training loss:  0.2868\n",
      "Epoch =  724 training loss:  0.2868\n",
      "Epoch =  725 training loss:  0.2868\n",
      "Epoch =  726 training loss:  0.2868\n",
      "Epoch =  727 training loss:  0.2868\n",
      "Epoch =  728 training loss:  0.2868\n",
      "Epoch =  729 training loss:  0.2868\n",
      "Epoch =  730 training loss:  0.2868\n",
      "Epoch =  731 training loss:  0.2868\n",
      "Epoch =  732 training loss:  0.2868\n",
      "Epoch =  733 training loss:  0.2868\n",
      "Epoch =  734 training loss:  0.2868\n",
      "Epoch =  735 training loss:  0.2868\n",
      "Epoch =  736 training loss:  0.2868\n",
      "Epoch =  737 training loss:  0.2868\n",
      "Epoch =  738 training loss:  0.2868\n",
      "Epoch =  739 training loss:  0.2868\n",
      "Epoch =  740 training loss:  0.2868\n",
      "Epoch =  741 training loss:  0.2868\n",
      "Epoch =  742 training loss:  0.2868\n",
      "Epoch =  743 training loss:  0.2868\n",
      "Epoch =  744 training loss:  0.2868\n",
      "Epoch =  745 training loss:  0.2868\n",
      "Epoch =  746 training loss:  0.2868\n",
      "Epoch =  747 training loss:  0.2868\n",
      "Epoch =  748 training loss:  0.2868\n",
      "Epoch =  749 training loss:  0.2868\n",
      "Epoch =  750 training loss:  0.2868\n",
      "Epoch =  751 training loss:  0.2868\n",
      "Epoch =  752 training loss:  0.2868\n",
      "Epoch =  753 training loss:  0.2868\n",
      "Epoch =  754 training loss:  0.2868\n",
      "Epoch =  755 training loss:  0.2868\n",
      "Epoch =  756 training loss:  0.2868\n",
      "Epoch =  757 training loss:  0.2868\n",
      "Epoch =  758 training loss:  0.286799\n",
      "Epoch =  759 training loss:  0.2868\n",
      "Epoch =  760 training loss:  0.2868\n",
      "Epoch =  761 training loss:  0.2868\n",
      "Epoch =  762 training loss:  0.2868\n",
      "Epoch =  763 training loss:  0.2868\n",
      "Epoch =  764 training loss:  0.2868\n",
      "Epoch =  765 training loss:  0.2868\n",
      "Epoch =  766 training loss:  0.2868\n",
      "Epoch =  767 training loss:  0.2868\n",
      "Epoch =  768 training loss:  0.2868\n",
      "Epoch =  769 training loss:  0.2868\n",
      "Epoch =  770 training loss:  0.286801\n",
      "Epoch =  771 training loss:  0.2868\n",
      "Epoch =  772 training loss:  0.286799\n",
      "Epoch =  773 training loss:  0.2868\n",
      "Epoch =  774 training loss:  0.2868\n",
      "Epoch =  775 training loss:  0.286799\n",
      "Epoch =  776 training loss:  0.286799\n",
      "Epoch =  777 training loss:  0.286799\n",
      "Epoch =  778 training loss:  0.2868\n",
      "Epoch =  779 training loss:  0.2868\n",
      "Epoch =  780 training loss:  0.2868\n",
      "Epoch =  781 training loss:  0.286801\n",
      "Epoch =  782 training loss:  0.2868\n",
      "Epoch =  783 training loss:  0.2868\n",
      "Epoch =  784 training loss:  0.2868\n",
      "Epoch =  785 training loss:  0.2868\n",
      "Epoch =  786 training loss:  0.2868\n",
      "Epoch =  787 training loss:  0.2868\n",
      "Epoch =  788 training loss:  0.286801\n",
      "Epoch =  789 training loss:  0.2868\n",
      "Epoch =  790 training loss:  0.2868\n",
      "Epoch =  791 training loss:  0.2868\n",
      "Epoch =  792 training loss:  0.286799\n",
      "Epoch =  793 training loss:  0.2868\n",
      "Epoch =  794 training loss:  0.2868\n",
      "Epoch =  795 training loss:  0.2868\n",
      "Epoch =  796 training loss:  0.2868\n",
      "Epoch =  797 training loss:  0.2868\n",
      "Epoch =  798 training loss:  0.2868\n",
      "Epoch =  799 training loss:  0.2868\n",
      "Epoch =  800 training loss:  0.2868\n",
      "Epoch =  801 training loss:  0.2868\n",
      "Epoch =  802 training loss:  0.2868\n",
      "Epoch =  803 training loss:  0.2868\n",
      "Epoch =  804 training loss:  0.2868\n",
      "Epoch =  805 training loss:  0.2868\n",
      "Epoch =  806 training loss:  0.2868\n",
      "Epoch =  807 training loss:  0.286799\n",
      "Epoch =  808 training loss:  0.2868\n",
      "Epoch =  809 training loss:  0.2868\n",
      "Epoch =  810 training loss:  0.2868\n",
      "Epoch =  811 training loss:  0.2868\n",
      "Epoch =  812 training loss:  0.286801\n",
      "Epoch =  813 training loss:  0.2868\n",
      "Epoch =  814 training loss:  0.286799\n",
      "Epoch =  815 training loss:  0.2868\n",
      "Epoch =  816 training loss:  0.2868\n",
      "Epoch =  817 training loss:  0.2868\n",
      "Epoch =  818 training loss:  0.2868\n",
      "Epoch =  819 training loss:  0.2868\n",
      "Epoch =  820 training loss:  0.2868\n",
      "Epoch =  821 training loss:  0.2868\n",
      "Epoch =  822 training loss:  0.2868\n",
      "Epoch =  823 training loss:  0.2868\n",
      "Epoch =  824 training loss:  0.2868\n",
      "Epoch =  825 training loss:  0.2868\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", shape=[None, d])\n",
    "y = tf.placeholder(\"float\", shape=[None, 1])\n",
    "\n",
    "# Weight initializations\n",
    "\n",
    "# w_1 = init_weights((d, k))\n",
    "# w_2 = []\n",
    "# v_choice = [1,-1]\n",
    "# for iter in range(k):\n",
    "#     w_2.append(random.choice(v_choice))\n",
    "# w_2 = tf.cast(tf.Variable(np.transpose(np.asarray([w_2])), trainable=False), tf.float32)\n",
    "\n",
    "# w_2 = init_weights((k, 1))\n",
    "\n",
    "w_1 = tf.Variable(np.transpose(tensorWeights[0]))\n",
    "# w_2 = tf.Variable(tensorWeights[1], trainable=False)\n",
    "w_2 = tf.Variable( np.transpose([v_gt]), trainable=False)\n",
    "\n",
    "w_1 = tf.cast(w_1, tf.float32)\n",
    "w_2 = tf.cast(w_2, tf.float32)\n",
    "\n",
    "# Forward propagation\n",
    "yhat  = forwardprop(X, w_1, w_2)\n",
    "\n",
    "# Backward propagation\n",
    "cost = tf.losses.mean_squared_error(y, yhat)\n",
    "updates = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# Run SGD\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "epsilon = 1e-4\n",
    "x_e = []\n",
    "y_tl = []\n",
    "T= 1000\n",
    "for epoch in range(T):\n",
    "    # Train with each example\n",
    "    i = 0\n",
    "#     print sess.run(w_1, feed_dict={X:train_x})\n",
    "#     print sess.run(w_2, feed_dict={X:train_x})\n",
    "    for iter in range(int(n/batch_size)):\n",
    "        sess.run(updates, feed_dict={X: train_x[i: i + batch_size], y: train_y[i: i + batch_size].reshape(batch_size,1)})\n",
    "        i = (i + batch_size)%n\n",
    "#     train_accuracy = np.mean((train_y - sess.run(yhat, feed_dict={X:train_x, y:train_y.reshape(n,1)})) <= epsilon)\n",
    "    loss = sess.run(cost, feed_dict={X:train_x, y:train_y.reshape(n,1)})\n",
    "    print \"Epoch = \", epoch+1,\"training loss: \", loss #,\" train Acc: \",100.*train_accuracy \n",
    "    x_e.append(epoch+1)\n",
    "    y_tl.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "permList = list(itertools.permutations(range(k)))\n",
    "\n",
    "# w_p = tensorWeights[0]\n",
    "# w_p = np.random.normal(0, 0.1 , (k,d))\n",
    "w_p = np.transpose(sess.run(w_1))\n",
    "# v_p = tensorWeights[1]\n",
    "v_p = sess.run(w_2)\n",
    "temp = []\n",
    "for i in v_p:\n",
    "    temp.append(i[0])\n",
    "v_p = np.asarray(temp)\n",
    "for perm in permList:\n",
    "    w_pi = w_p[list(perm)]\n",
    "    v_pi = v_p[list(perm)]\n",
    "    \n",
    "    w_gt = np.transpose(W_gt)\n",
    "    \n",
    "    if sum(v_gt == v_pi) == k:\n",
    "        max_diff = 0\n",
    "        for i in range(k):\n",
    "            diff = np.linalg.norm(w_pi[i]-w_gt[i])/np.linalg.norm(w_gt)\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff\n",
    "        if max_diff < 0.5:\n",
    "            print \"Success\"\n",
    "            break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAARiCAYAAADVzyypAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81bd435610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_s = y_tl\n",
    "plt.plot(x_e, y_tl)\n",
    "plt.plot(x_e, y_s)\n",
    "plt.rcParams[\"figure.figsize\"] = [20,20]\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-py2.7-mat]",
   "language": "python",
   "name": "conda-env-tf-py2.7-mat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
