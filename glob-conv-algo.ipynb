{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random, math\n",
    "import matlab\n",
    "import matlab.engine as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyper-Parameters\n",
    "n = 2000 # number of samples\n",
    "d = 30  # input dimension\n",
    "k = 5   # hidden layer size\n",
    "kappa = 2 \n",
    "lr = 0.02\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Activation Function\n",
    "def phi_s(h):\n",
    "    return h**2 if h > 0 else 0\n",
    "phi = np.vectorize(phi_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data and Params Generation\n",
    "\n",
    "gauss_mat_u = np.random.normal(0.0, 0.1 , (d,k))\n",
    "gauss_mat_v = np.random.normal(0.0, 0.1 , (k,k))\n",
    "\n",
    "U, temp = np.linalg.qr(gauss_mat_u)\n",
    "V, temp = np.linalg.qr(gauss_mat_v)\n",
    "\n",
    "# U, V = np.linalg.qr(gauss_mat_u)\n",
    "diag = []\n",
    "v_gt = []\n",
    "v_choice = [1,-1]\n",
    "for iter in range(k):\n",
    "    diag.append(1+1.*iter*(kappa-1)/(k-1))\n",
    "    v_gt.append(random.choice(v_choice))\n",
    "    \n",
    "Sigma = np.diag(diag)\n",
    "W_gt = np.dot(np.dot(U, Sigma), np.transpose(V))\n",
    "v_gt = np.asarray(v_gt)\n",
    "train_x = []\n",
    "train_y = []\n",
    "for iter in range(n):\n",
    "    train_x.append(np.random.normal(0.0,1.,d))\n",
    "    train_y.append(np.dot(phi(np.dot(train_x[iter], W_gt)),v_gt))\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.transpose(np.asarray(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## moments\n",
    "\n",
    "def gamma(j,sigma):\n",
    "    estm = 0\n",
    "    for i in range(10000):\n",
    "        z = np.random.normal(0.0, 1)\n",
    "        estm += phi_s(z*sigma)*np.power(z,j)\n",
    "    return estm/10000\n",
    "\n",
    "m = np.zeros((4,k))\n",
    "for i in range(k):\n",
    "    m[0,i] = gamma(1,np.linalg.norm(W_gt[:,i]))\n",
    "    m[1,i] = gamma(2,np.linalg.norm(W_gt[:,i])) - gamma(0,np.linalg.norm(W_gt[:,i]))\n",
    "    m[2,i] = gamma(3,np.linalg.norm(W_gt[:,i])) - 3*gamma(1,np.linalg.norm(W_gt[:,i]))\n",
    "    m[3,i] = gamma(4,np.linalg.norm(W_gt[:,i])) + 3*gamma(0,np.linalg.norm(W_gt[:,i])) - 6*gamma(2,np.linalg.norm(W_gt[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outer3(a,b,c):\n",
    "    A = np.outer(a,b)\n",
    "    B = []\n",
    "    for third in c:\n",
    "        B.append(A*third)\n",
    "    return np.asarray(B)\n",
    "\n",
    "def outer4(a,b,c,d):\n",
    "    A = outer3(a,b,c)\n",
    "    B = []\n",
    "    for fourth in d:\n",
    "        B.append(A*fourth)\n",
    "    return np.asarray(B)\n",
    "\n",
    "def outer3I(x):\n",
    "    return outer3(x,x,x)\n",
    "\n",
    "def outer4I(x):\n",
    "    return outer4(x,x,x,x)\n",
    "\n",
    "def specOuterI(x):\n",
    "    d = len(x)\n",
    "    iden = np.identity(d)\n",
    "    final = np.zeros([d,d,d])\n",
    "    for i in range(d):\n",
    "        final += outer3(x, iden[i], iden[i]) + outer3(iden[i], x, iden[i])+ outer3(iden[i], iden[i],x)\n",
    "    return final\n",
    "\n",
    "def specOuterMat(M):\n",
    "    d = np.shape(M)[0]\n",
    "    ## TODO\n",
    "    return np.zeros([d,d,d,d])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1177824429409902"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[2,3]/np.linalg.norm(W_gt[:,3])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multLnr(M, argList):\n",
    "    if len(np.shape(M)) == 3:\n",
    "        a,b,c = argList\n",
    "\n",
    "        assert(np.shape(M)[0] == np.shape(a)[0])\n",
    "        assert(np.shape(M)[1] == np.shape(b)[0])\n",
    "        assert(np.shape(M)[2] == np.shape(c)[0])\n",
    "        ## HardCoding Here :: TODO\n",
    "        res = np.zeros([np.shape(a)[-1],np.shape(b)[-1] ])\n",
    "        for itera in range(np.shape(a)[0]):\n",
    "            for iterb in range(np.shape(b)[0]):\n",
    "                for iterc in range(np.shape(c)[0]):\n",
    "                    res += M[itera, iterb, iterc]*c[iterc]*np.outer(a[itera], b[iterb])\n",
    "        return res\n",
    "    else:\n",
    "        assert(len(np.shape(M)) == 4)\n",
    "        a,b,c,d = argList\n",
    "        if len(np.shape(c)) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            res = np.zeros([np.shape(a)[-1],np.shape(b)[-1] ])\n",
    "            for itera in range(np.shape(a)[0]):\n",
    "                for iterb in range(np.shape(b)[0]):\n",
    "                    for iterc in range(np.shape(c)[0]):\n",
    "                        for iterd in range(np.shape(d)[0]):\n",
    "                            res += M[itera, iterb, iterc]*c[iterc]*d[iterd]*np.outer(a[itera], b[iterb])\n",
    "            return res\n",
    "        \n",
    "def multLnr1(M, V):\n",
    "    d = np.shape(M)[0]\n",
    "    k = np.shape(V)[1]\n",
    "    \n",
    "    res = np.zeros((k,k))\n",
    "    \n",
    "    for itera in range(d):\n",
    "        for iterb in range(d):\n",
    "            res += M[itera, iterb]*np.outer(V[itera], V[iterb])\n",
    "            \n",
    "    return res\n",
    "\n",
    "def multLnr2(P3, V):\n",
    "    d = np.shape(P3)[0]\n",
    "    k = np.shape(V)[1]\n",
    "    \n",
    "    res = np.zeros((k,k,k))\n",
    "    \n",
    "    for itera in range(d):\n",
    "        for iterb in range(d):\n",
    "            for iterc in range(d):\n",
    "                res += P3[itera, iterb, iterc]*outer3(V[itera], V[iterb], V[iterc])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def prob(x):\n",
    "#     return math.pow(math.e, -0.5*np.dot(np.transpose(x),x))/np.sqrt(math.pow(2*math.pi, len(x)))\n",
    "\n",
    "def getM1(X, y):\n",
    "    M1 = np.zeros(d)\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M1 += y[iter]*X[iter]\n",
    "    return M1/len(y)\n",
    "\n",
    "def getM2(X, y):\n",
    "    M2 = np.zeros([d,d])\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M2 += y[iter]*(np.outer(X[iter], X[iter]) - np.identity(d))\n",
    "    return M2/len(y)\n",
    "\n",
    "def getM3(X, y):\n",
    "    M3 = np.zeros([d,d,d])\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M3 += y[iter]*(outer3I(X[iter]) - specOuterI(X[iter]) )\n",
    "    return M3/len(y)\n",
    "\n",
    "def getM4(X, y):\n",
    "    M4 = np.zeros([d,d,d,d])\n",
    "    for iter in range(np.shape(X)[0]):\n",
    "        M4 += y[iter]*(outer4I(X[iter]) - specOuterMat(np.outer(X[iter], X[iter])) + specOuterMat(np.identity(d)) )\n",
    "    return M4/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getP2V(V, X, y, k):\n",
    "    d = np.shape(X)[1]\n",
    "    P2V = np.zeros((d,k))\n",
    "    for i in range(len(X)):\n",
    "        P2V += y[i]*(np.dot(np.transpose([X[i]]), np.dot([X[i]], V) ) - V)\n",
    "    return P2V/np.shape(X)[0]\n",
    "        \n",
    "def getP2v(v, X, y):\n",
    "    d = np.shape(X)[1]\n",
    "    P2v = np.zeros((d,1))\n",
    "    for i in range(len(X)):\n",
    "        P2v += y[i]*(np.transpose([X[i]])*np.dot([X[i]], np.transpose([v]))  - np.transpose([v]))\n",
    "    return P2v/np.shape(X)[0]\n",
    "\n",
    "def topk(eigenV, k):\n",
    "    sortList = []\n",
    "    for i in range(2):\n",
    "        for j in range(k):\n",
    "            sortList.append([eigenV[i,j], (i,j)])\n",
    "    sortList.sort(reverse=True)\n",
    "    k1 = 0\n",
    "    k2 = 0\n",
    "    pi1 = {}\n",
    "    pi2 = {}\n",
    "    for i in range(k):\n",
    "        if sortList[i][1][0] == 0:\n",
    "            pi1[k1] = sortList[i][1][1]\n",
    "            k1 += 1\n",
    "        else:\n",
    "            pi2[k2] = sortList[i][1][1]\n",
    "            k2 += 1\n",
    "    return pi1, pi2, k1, k2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def powMeth( k, X, y):\n",
    "#     C = 3*np.linalg.norm(P2)\n",
    "    C = 5\n",
    "    T = 10\n",
    "    d =  np.shape(X)[1]\n",
    "    V1 = np.random.normal(0.0, 0.1 , (d,k))\n",
    "    V2 = np.random.normal(0.0, 0.1 , (d,k))\n",
    "    \n",
    "    for i in range(T):\n",
    "        P2V1 = getP2V(V1, X, y, k)\n",
    "        P2V2 = getP2V(V2, X, y, k)\n",
    "        V1, temp = np.linalg.qr(C*V1 + P2V1)\n",
    "        V2, temp = np.linalg.qr(C*V2 - P2V2)\n",
    "    \n",
    "    eigenV = np.zeros((2,k))\n",
    "    for i in range(k):\n",
    "        eigenV[0,i] = abs( np.dot(np.transpose(V1[:,i]) , getP2v(V1[:,i], X, y) ) )\n",
    "    for i in range(k):\n",
    "        eigenV[1,i] = abs(np.dot(np.transpose(V2[:,i]) , getP2v(V2[:,i], X, y) ))\n",
    "    \n",
    "    pi1, pi2, k1, k2 = topk(eigenV, k)\n",
    "    \n",
    "    V1_new = np.zeros((d,k1))\n",
    "    V2_new = np.zeros((d,k2))\n",
    "    \n",
    "    for i in range(k1):\n",
    "        V1_new[:,i] = V1[:,pi1[i]]\n",
    "    for i in range(k2):\n",
    "        V2_new[:,i] = V2[:,pi2[i]]\n",
    "    V2_new, temp = np.linalg.qr(np.dot(np.identity(d)-np.dot(V1_new, np.transpose(V1_new)), V2_new))\n",
    "    return np.concatenate((V1_new, V2_new), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recmagsgn(V, U, X, y):\n",
    "    \n",
    "    ### l1 and l2 :: TODO\n",
    "    l1 = 1\n",
    "    l2 = 2\n",
    "    \n",
    "    d = np.shape(X)[1]\n",
    "    k = np.shape(U)[0]\n",
    "    divInd = int(len(X)/2)\n",
    "    \n",
    "    # Partition\n",
    "    X1 = X[:divInd]\n",
    "    y1 = y[:divInd]\n",
    "\n",
    "    X2 = X[divInd:]\n",
    "    y2 = y[divInd:]\n",
    "    \n",
    "    alpha = np.random.normal(0.0, 0.1 , d)\n",
    "    \n",
    "    ## TODO: Assuming l1=1 and l2=2 \n",
    "    Q1 = getM1(X1, y1)\n",
    "    Q2 = multLnr1(getM2(X2,y2), V).flatten()    \n",
    "    \n",
    "    Vu = []\n",
    "    UU = []\n",
    "\n",
    "    for ind in range(len(U)):\n",
    "        Vu.append(np.dot(V, U[ind]))\n",
    "        UU.append(np.dot( np.transpose([U[ind]]), [U[ind]] ).flatten())\n",
    "        \n",
    "    ## Estimating z\n",
    "    z_old = np.zeros(k)\n",
    "    z_new = np.zeros(k)\n",
    "    \n",
    "    T = 1000\n",
    "    for iterT in range(T):\n",
    "        for ind in range(k):\n",
    "            \n",
    "            mult_fact = np.zeros(d)\n",
    "            for j in range(k):\n",
    "                if j != ind:\n",
    "                    mult_fact += z_old[j]*Vu[j]\n",
    "            div_fact = np.dot(np.transpose(Vu[ind]), Vu[ind])\n",
    "            z_new[ind] = (np.dot(np.transpose(Q1), Vu[ind]) + np.dot(np.transpose(Vu[ind]), Q1) -\n",
    "                                 np.dot(np.transpose(Vu[ind]), mult_fact) - \n",
    "                                  np.dot(np.transpose(mult_fact), Vu[ind]) )/(2*div_fact)\n",
    "        z_old = z_new\n",
    "    \n",
    "    ## Estimating r\n",
    "    r = np.dot(np.linalg.inv(np.dot(UU, np.transpose(UU))), \n",
    "                                                   np.dot(UU,np.transpose([Q2]) ) )\n",
    "    v = np.sign(r*np.transpose([m[l2-1]]))\n",
    "    s = np.sign(v*np.transpose([z_new])*np.transpose([m[l2-1]]))\n",
    "\n",
    "    p = 1 ## p + 1 is degree of homogenity\n",
    "\n",
    "    w = []\n",
    "    for ind in range(k):\n",
    "        \n",
    "        w.append(s[ind]*np.math.pow(abs(z_new[ind]*np.linalg.norm(W_gt[:,ind])**(p+1)/(m[l1-1,ind]*np.math.pow( np.dot( [alpha],\n",
    "                    np.transpose([Vu[ind]]) ) ,l1-1))), 1.0/(p+1))*Vu[ind])\n",
    "    w = np.asarray(w)\n",
    "\n",
    "    return w,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensorInit(X, y):\n",
    "    divInd = int(len(X)/3)\n",
    "    \n",
    "    # Partition\n",
    "    X1 = X[:divInd]\n",
    "    y1 = y[:divInd]\n",
    "\n",
    "    X2 = X[divInd:2*divInd]\n",
    "    y2 = y[divInd:2*divInd]\n",
    "    \n",
    "    X3 = X[2*divInd:]\n",
    "    y3 = y[2*divInd:]\n",
    "    \n",
    "    ## P2 \n",
    "    ## Estimating P2 as M2 :: TODO\n",
    "    \n",
    "#     alpha = np.random.normal(0.0, 0.1 , d)\n",
    "#     P2 = getM2(X1,y1)\n",
    "#     print(np.linalg.norm(P2))\n",
    "\n",
    "    ## Power Method\n",
    "    V = powMeth(k, X1, y1)\n",
    "    \n",
    "    ## R3\n",
    "    R3 = multLnr2(getM3(X2, y2), V)\n",
    "    \n",
    "    #### KCL\n",
    "    R = matlab.double(R3.flatten().tolist())\n",
    "    eng = me.start_matlab()\n",
    "    U = eng.notf_frompy(R, 100, k)\n",
    "    eng.quit()\n",
    "    U = np.asarray(U)\n",
    "#     U = R3[0]\n",
    "\n",
    "    ## RecMagSign\n",
    "    return recmagsgn(V, U, X3, y3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1) (5, 1) (5,) (5, 1)\n"
     ]
    }
   ],
   "source": [
    "tensorWeights = tensorInit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.37392581, -0.05092418, -0.35151054,  0.13012054,  0.22694875,\n",
       "          0.29857479, -0.39269674,  0.23516854, -0.59599874, -0.24977239,\n",
       "         -0.35027368, -0.03116442,  0.12362358, -0.12626356, -0.33334929,\n",
       "         -0.22627306, -0.03852031,  0.16972871, -0.15833608, -0.43697969,\n",
       "          0.2056168 ,  0.38228941,  0.33090043,  0.14985998,  0.04673024,\n",
       "          0.5108823 , -0.16337391,  0.38377643, -0.20215343, -0.14212017],\n",
       "        [-0.31826157, -0.45356814, -0.07006219,  0.30922661,  0.28271523,\n",
       "          0.15626499, -0.17005251,  0.3328327 ,  0.12368895, -0.05381514,\n",
       "          0.40220636, -0.12492175, -0.12485536,  0.47862001,  0.05847642,\n",
       "         -0.45148084, -0.03004689, -0.04883601, -0.6686968 ,  0.02822519,\n",
       "          0.04476548,  0.17929831, -0.17350899,  0.78722281, -0.47986722,\n",
       "          0.13989857,  0.23600322,  0.2258645 , -0.08957214,  0.21453945],\n",
       "        [-0.18948587,  0.01539093, -0.00495063, -0.09347432,  0.30542428,\n",
       "         -0.0066048 ,  0.0449505 ,  0.33228498, -0.12477255, -0.03626806,\n",
       "          0.00968619,  0.41500584, -0.09471123, -0.03847311,  0.15470606,\n",
       "          0.43408219,  0.01449276, -0.28558707, -0.1527315 , -0.03196292,\n",
       "         -0.08245342,  0.07355308,  0.55733255, -0.20620819, -0.09633808,\n",
       "          0.13711555, -0.12244811,  0.14841003, -0.27838711,  0.54005986],\n",
       "        [ 0.1774347 , -0.04041959, -0.16824725,  0.11713895, -0.02841525,\n",
       "          0.05758005, -0.01465657,  0.11810241,  0.19459413,  0.08644305,\n",
       "          0.08764685,  0.18394786, -0.03054897,  0.10216952, -0.01475512,\n",
       "          0.00716909, -0.10587562, -0.01605481,  0.05068831, -0.08660365,\n",
       "          0.0267368 , -0.06867761,  0.05100311,  0.17194498, -0.07711722,\n",
       "          0.13624952,  0.03460615, -0.10688679, -0.24712005, -0.03047454],\n",
       "        [-0.11294692, -0.36233258, -0.27226429,  0.04485227, -0.04873488,\n",
       "          0.05397958,  0.29811647,  0.16260239,  0.22333625,  0.06254044,\n",
       "         -0.09572986, -0.35279256, -0.32832146,  0.47259037,  0.31555062,\n",
       "         -0.12829292, -0.03029614,  0.11097608,  0.26029276, -0.04283828,\n",
       "          0.11217242, -0.22957776,  0.08966564,  0.70652286, -0.26625672,\n",
       "          0.62159943,  0.26132336,  0.22577022,  0.07853543,  0.20657816]]),\n",
       " array([[-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    weights = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def forwardprop(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Forward-propagation.\n",
    "    \"\"\"\n",
    "    h    = tf.square(tf.nn.relu((tf.matmul(X, w_1))))\n",
    "    yhat = tf.matmul(h, w_2)  # The \\varphi function\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "permList = list(itertools.permutations(range(k)))\n",
    "\n",
    "w_p = tensorWeights[0]\n",
    "v_p = tensorWeights[1]\n",
    "temp = []\n",
    "for i in v_p:\n",
    "    temp.append(i[0])\n",
    "v_p = np.asarray(temp)\n",
    "for perm in permList:\n",
    "    w_pi = w_p[list(perm)]\n",
    "    v_pi = v_p[list(perm)]\n",
    "    \n",
    "    w_gt = np.transpose(W_gt)\n",
    "    \n",
    "    if sum(v_gt == v_pi) == k:\n",
    "        max_diff = 0\n",
    "        for i in range(k):\n",
    "            diff = np.linalg.norm(w_pi[i]-w_gt[i])/np.linalg.norm(w_gt)\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff\n",
    "        if max_diff < 0.01:\n",
    "            print \"Success\"\n",
    "            break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch =  1 training loss:  0.214009\n",
      "Epoch =  2 training loss:  0.152727\n",
      "Epoch =  3 training loss:  0.142175\n",
      "Epoch =  4 training loss:  0.137376\n",
      "Epoch =  5 training loss:  0.134423\n",
      "Epoch =  6 training loss:  0.132107\n",
      "Epoch =  7 training loss:  0.130012\n",
      "Epoch =  8 training loss:  0.128015\n",
      "Epoch =  9 training loss:  0.126099\n",
      "Epoch =  10 training loss:  0.124205\n",
      "Epoch =  11 training loss:  0.122304\n",
      "Epoch =  12 training loss:  0.120354\n",
      "Epoch =  13 training loss:  0.118314\n",
      "Epoch =  14 training loss:  0.116207\n",
      "Epoch =  15 training loss:  0.114153\n",
      "Epoch =  16 training loss:  0.112354\n",
      "Epoch =  17 training loss:  0.110941\n",
      "Epoch =  18 training loss:  0.109895\n",
      "Epoch =  19 training loss:  0.109111\n",
      "Epoch =  20 training loss:  0.108508\n",
      "Epoch =  21 training loss:  0.108034\n",
      "Epoch =  22 training loss:  0.107648\n",
      "Epoch =  23 training loss:  0.107321\n",
      "Epoch =  24 training loss:  0.107037\n",
      "Epoch =  25 training loss:  0.106797\n",
      "Epoch =  26 training loss:  0.106591\n",
      "Epoch =  27 training loss:  0.106406\n",
      "Epoch =  28 training loss:  0.106237\n",
      "Epoch =  29 training loss:  0.106083\n",
      "Epoch =  30 training loss:  0.105941\n",
      "Epoch =  31 training loss:  0.105809\n",
      "Epoch =  32 training loss:  0.105683\n",
      "Epoch =  33 training loss:  0.105561\n",
      "Epoch =  34 training loss:  0.105443\n",
      "Epoch =  35 training loss:  0.105331\n",
      "Epoch =  36 training loss:  0.105223\n",
      "Epoch =  37 training loss:  0.10512\n",
      "Epoch =  38 training loss:  0.105021\n",
      "Epoch =  39 training loss:  0.104926\n",
      "Epoch =  40 training loss:  0.104835\n",
      "Epoch =  41 training loss:  0.104748\n",
      "Epoch =  42 training loss:  0.104663\n",
      "Epoch =  43 training loss:  0.104582\n",
      "Epoch =  44 training loss:  0.104504\n",
      "Epoch =  45 training loss:  0.104428\n",
      "Epoch =  46 training loss:  0.104356\n",
      "Epoch =  47 training loss:  0.104286\n",
      "Epoch =  48 training loss:  0.104218\n",
      "Epoch =  49 training loss:  0.104153\n",
      "Epoch =  50 training loss:  0.10409\n",
      "Epoch =  51 training loss:  0.104029\n",
      "Epoch =  52 training loss:  0.10397\n",
      "Epoch =  53 training loss:  0.103914\n",
      "Epoch =  54 training loss:  0.103858\n",
      "Epoch =  55 training loss:  0.103805\n",
      "Epoch =  56 training loss:  0.103753\n",
      "Epoch =  57 training loss:  0.103703\n",
      "Epoch =  58 training loss:  0.103654\n",
      "Epoch =  59 training loss:  0.103607\n",
      "Epoch =  60 training loss:  0.103561\n",
      "Epoch =  61 training loss:  0.103516\n",
      "Epoch =  62 training loss:  0.103473\n",
      "Epoch =  63 training loss:  0.10343\n",
      "Epoch =  64 training loss:  0.103389\n",
      "Epoch =  65 training loss:  0.103349\n",
      "Epoch =  66 training loss:  0.10331\n",
      "Epoch =  67 training loss:  0.103271\n",
      "Epoch =  68 training loss:  0.103234\n",
      "Epoch =  69 training loss:  0.103197\n",
      "Epoch =  70 training loss:  0.103162\n",
      "Epoch =  71 training loss:  0.103127\n",
      "Epoch =  72 training loss:  0.103092\n",
      "Epoch =  73 training loss:  0.103059\n",
      "Epoch =  74 training loss:  0.103026\n",
      "Epoch =  75 training loss:  0.102994\n",
      "Epoch =  76 training loss:  0.102963\n",
      "Epoch =  77 training loss:  0.102933\n",
      "Epoch =  78 training loss:  0.102903\n",
      "Epoch =  79 training loss:  0.102874\n",
      "Epoch =  80 training loss:  0.102846\n",
      "Epoch =  81 training loss:  0.102818\n",
      "Epoch =  82 training loss:  0.102791\n",
      "Epoch =  83 training loss:  0.102764\n",
      "Epoch =  84 training loss:  0.102738\n",
      "Epoch =  85 training loss:  0.102713\n",
      "Epoch =  86 training loss:  0.102688\n",
      "Epoch =  87 training loss:  0.102664\n",
      "Epoch =  88 training loss:  0.10264\n",
      "Epoch =  89 training loss:  0.102617\n",
      "Epoch =  90 training loss:  0.102594\n",
      "Epoch =  91 training loss:  0.102572\n",
      "Epoch =  92 training loss:  0.10255\n",
      "Epoch =  93 training loss:  0.102529\n",
      "Epoch =  94 training loss:  0.102509\n",
      "Epoch =  95 training loss:  0.102489\n",
      "Epoch =  96 training loss:  0.102469\n",
      "Epoch =  97 training loss:  0.10245\n",
      "Epoch =  98 training loss:  0.102431\n",
      "Epoch =  99 training loss:  0.102413\n",
      "Epoch =  100 training loss:  0.102395\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", shape=[None, d])\n",
    "y = tf.placeholder(\"float\", shape=[None, 1])\n",
    "\n",
    "# Weight initializations\n",
    "\n",
    "# w_1 = init_weights((d, k))\n",
    "# w_2 = []\n",
    "# v_choice = [1,-1]\n",
    "# for iter in range(k):\n",
    "#     w_2.append(random.choice(v_choice))\n",
    "# w_2 = tf.cast(tf.Variable(np.transpose(np.asarray([w_2]))), tf.float32)\n",
    "\n",
    "# w_2 = init_weights((k, 1))\n",
    "\n",
    "w_1 = tf.Variable(np.transpose(tensorWeights[0]))\n",
    "w_2 = tf.Variable(tensorWeights[1])\n",
    "\n",
    "w_1 = tf.cast(w_1, tf.float32)\n",
    "w_2 = tf.cast(w_2, tf.float32)\n",
    "\n",
    "# Forward propagation\n",
    "yhat  = forwardprop(X, w_1, w_2)\n",
    "\n",
    "# Backward propagation\n",
    "cost = tf.losses.mean_squared_error(y, yhat)\n",
    "updates = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "# Run SGD\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "epsilon = 1e-4\n",
    "x_e = []\n",
    "y_tl = []\n",
    "T= 100\n",
    "for epoch in range(T):\n",
    "    # Train with each example\n",
    "    i = 0\n",
    "#     print sess.run(w_1, feed_dict={X:train_x})\n",
    "#     print sess.run(w_2, feed_dict={X:train_x})\n",
    "    for iter in range(int(n/batch_size)):\n",
    "        sess.run(updates, feed_dict={X: train_x[i: i + batch_size], y: train_y[i: i + batch_size].reshape(batch_size,1)})\n",
    "        i = (i + batch_size)%n\n",
    "#     train_accuracy = np.mean((train_y - sess.run(yhat, feed_dict={X:train_x, y:train_y.reshape(n,1)})) <= epsilon)\n",
    "    loss = sess.run(cost, feed_dict={X:train_x, y:train_y.reshape(n,1)})\n",
    "    print \"Epoch = \", epoch+1,\"training loss: \", loss #,\" train Acc: \",100.*train_accuracy \n",
    "    x_e.append(epoch+1)\n",
    "    y_tl.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJCCAYAAAAC4omSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/n+iM9mLTJbP7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c6328ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_s = y_tl\n",
    "plt.plot(x_e, y_tl)\n",
    "plt.plot(x_e, y_s)\n",
    "plt.rcParams[\"figure.figsize\"] = [20,20]\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
